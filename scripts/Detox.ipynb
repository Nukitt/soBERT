{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk import ngrams\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device('cuda:0')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m BertForMaskedLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:1966\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1964\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   1965\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 1966\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   1967\u001b[0m         config_path,\n\u001b[1;32m   1968\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1969\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1970\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1971\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1972\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1973\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1974\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1975\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1976\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1977\u001b[0m         _from_auto\u001b[39m=\u001b[39;49mfrom_auto_class,\n\u001b[1;32m   1978\u001b[0m         _from_pipeline\u001b[39m=\u001b[39;49mfrom_pipeline,\n\u001b[1;32m   1979\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1980\u001b[0m     )\n\u001b[1;32m   1981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1982\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/configuration_utils.py:532\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[39m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    533\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m    534\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    535\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/configuration_utils.py:559\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    558\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    561\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/configuration_utils.py:614\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    612\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    615\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    616\u001b[0m         configuration_file,\n\u001b[1;32m    617\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    618\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    619\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    620\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    621\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    622\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    623\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    624\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    625\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    626\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    627\u001b[0m     )\n\u001b[1;32m    628\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    629\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1053\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1052\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1054\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1055\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1056\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1057\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1058\u001b[0m         )\n\u001b[1;32m   1059\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1060\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1062\u001b[0m             HUGGINGFACE_HEADER_X_REPO_COMMIT\n\u001b[1;32m   1063\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1350\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, use_auth_token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1347\u001b[0m headers \u001b[39m=\u001b[39m build_hf_headers(use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[1;32m   1349\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1350\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1351\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1352\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1353\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1354\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1355\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1356\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1357\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1358\u001b[0m )\n\u001b[1;32m   1359\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1361\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:398\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 398\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    399\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    400\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    401\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    402\u001b[0m         base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    403\u001b[0m         max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    404\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    405\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    406\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    409\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:433\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m    432\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[1;32m    434\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    435\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    436\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    437\u001b[0m     base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    438\u001b[0m     max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    439\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39;49m(ConnectTimeout, ProxyError),\n\u001b[1;32m    440\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    441\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    442\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    443\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:105\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m nb_tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    106\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    107\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/urllib3/connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    406\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs\n\u001b[1;32m    407\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(context, \u001b[39m\"\u001b[39m\u001b[39mload_default_certs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    415\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    416\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    417\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    418\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    419\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    420\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    421\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    422\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    423\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    424\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    425\u001b[0m )\n\u001b[1;32m    427\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    431\u001b[0m     default_ssl_context\n\u001b[1;32m    432\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock, \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    434\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock\u001b[39m.\u001b[39mversion() \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mTLSv1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTLSv1.1\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    435\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    437\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         SNIMissingWarning,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    514\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    515\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    516\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    517\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    518\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    519\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    520\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    521\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/ssl.py:1071\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m   1069\u001b[0m             \u001b[39m# non-blocking\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1072\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1073\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m block:\n\u001b[1;32m   1341\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1343\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a list of negative and positive words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give a toxicity and normalcy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NgramSalienceCalculator():\n",
    "    def __init__(self, tox_corpus, norm_corpus, use_ngrams=False):\n",
    "        ngrams = (1, 3) if use_ngrams else (1, 1)\n",
    "        self.vectorizer = CountVectorizer(ngram_range=ngrams)\n",
    "\n",
    "        tox_count_matrix = self.vectorizer.fit_transform(tox_corpus)\n",
    "        self.tox_vocab = self.vectorizer.vocabulary_\n",
    "        self.tox_counts = np.sum(tox_count_matrix, axis=0)\n",
    "\n",
    "        norm_count_matrix = self.vectorizer.fit_transform(norm_corpus)\n",
    "        self.norm_vocab = self.vectorizer.vocabulary_\n",
    "        self.norm_counts = np.sum(norm_count_matrix, axis=0)\n",
    "\n",
    "    def salience(self, feature, attribute='tox', lmbda=0.5):\n",
    "        assert attribute in ['tox', 'norm']\n",
    "        if feature not in self.tox_vocab:\n",
    "            tox_count = 0.0\n",
    "        else:\n",
    "            tox_count = self.tox_counts[0, self.tox_vocab[feature]]\n",
    "\n",
    "        if feature not in self.norm_vocab:\n",
    "            norm_count = 0.0\n",
    "        else:\n",
    "            norm_count = self.norm_counts[0, self.norm_vocab[feature]]\n",
    "\n",
    "        if attribute == 'tox':\n",
    "            return (tox_count + lmbda) / (norm_count + lmbda)\n",
    "        else:\n",
    "            return (norm_count + lmbda) / (tox_count + lmbda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88645\n"
     ]
    }
   ],
   "source": [
    "tox_corpus_path = '../data/train/train_toxic'\n",
    "norm_corpus_path = '../data/train/train_normal'\n",
    "\n",
    "for fn in [tox_corpus_path, norm_corpus_path]:\n",
    "    with open(fn, 'r') as corpus:\n",
    "        for line in corpus.readlines():\n",
    "            for tok in line.strip().split():\n",
    "                c[tok] += 1\n",
    "\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88645\n"
     ]
    }
   ],
   "source": [
    "vocab = {w for w, _ in c.most_common() if _ > 0}  # if we took words with > 1 occurences, vocabulary would be x2 smaller, but we'll survive this size\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tox_corpus_path, 'r') as tox_corpus, open(norm_corpus_path, 'r') as norm_corpus:\n",
    "    corpus_tox = [' '.join([w if w in vocab else '<unk>' for w in line.strip().split()]) for line in tox_corpus.readlines()]\n",
    "    corpus_norm = [' '.join([w if w in vocab else '<unk>' for w in line.strip().split()]) for line in norm_corpus.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 4\n",
    "\n",
    "sc = NgramSalienceCalculator(corpus_tox, corpus_norm, False)\n",
    "seen_grams = set()\n",
    "\n",
    "neg_out_name = '../data/vocab/negative_words.txt'\n",
    "pos_out_name = '../data/vocab/positive_words.txt'\n",
    "\n",
    "with open(neg_out_name, 'a') as neg_out, open(pos_out_name, 'a') as pos_out:\n",
    "    for gram in set(sc.tox_vocab.keys()).union(set(sc.norm_vocab.keys())):\n",
    "        if gram not in seen_grams:\n",
    "            seen_grams.add(gram)\n",
    "            toxic_salience = sc.salience(gram, attribute='tox')\n",
    "            polite_salience = sc.salience(gram, attribute='norm')\n",
    "            if toxic_salience > threshold:\n",
    "                neg_out.writelines(f'{gram}\\n')\n",
    "            elif polite_salience > threshold:\n",
    "                pos_out.writelines(f'{gram}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression for evaluating word toxicities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/train/train_toxic\", 'r') as f:\n",
    "    toxic = f.readlines()\n",
    "\n",
    "with open(\"../data/train/train_normal\", 'r') as f:\n",
    "    normal = f.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135390"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135390"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a txt in a list, every line a new element\n",
    "def read_txt(path):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # remove \\n\n",
    "    lines = [line.strip() for line in lines]\n",
    "    return lines\n",
    "\n",
    "negative = read_txt(\"../data/vocab/negative_words.txt\")\n",
    "positive = read_txt(\"../data/vocab/positive_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to pandas\n",
    "toxic_df = pd.DataFrame(toxic, columns=['text'])\n",
    "normal_df = pd.DataFrame(normal, columns=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just a comment regarding family trusts , they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nor do they conform to the notion that our tit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yeah , the pers employees and their pensions f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why risk our marine parks and sea life for a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not sure what flavor koolaid you drinking but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135385</th>\n",
       "      <td>i think you , other like minded individuals ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135386</th>\n",
       "      <td>a lot of people will be wondering what they we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135387</th>\n",
       "      <td>they have endured saber rattling by the us for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135388</th>\n",
       "      <td>the cbc has been totally silent on reporting a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135389</th>\n",
       "      <td>its not the police fault .\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135390 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "0       just a comment regarding family trusts , they ...\n",
       "1       nor do they conform to the notion that our tit...\n",
       "2       yeah , the pers employees and their pensions f...\n",
       "3       why risk our marine parks and sea life for a f...\n",
       "4       not sure what flavor koolaid you drinking but ...\n",
       "...                                                   ...\n",
       "135385  i think you , other like minded individuals ge...\n",
       "135386  a lot of people will be wondering what they we...\n",
       "135387  they have endured saber rattling by the us for...\n",
       "135388  the cbc has been totally silent on reporting a...\n",
       "135389                       its not the police fault .\\n\n",
       "\n",
       "[135390 rows x 1 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "\n",
    "def masking(df, word_list):\n",
    "    df['tokenized_text'] = df['text'].apply((lambda x: tokenizer.tokenize(x)))\n",
    "    df['masked_text'] = df['tokenized_text'].apply((lambda x: [word.lower() if word not in word_list else '[MASK]' for word in x]))\n",
    "    df[\"masked_encoded_text\"] = df['masked_text'].apply((lambda x: tokenizer.encode(x,)))\n",
    "    df[\"encoded_text\"] = df['tokenized_text'].apply((lambda x: tokenizer.encode(x)))\n",
    "    return df\n",
    "\n",
    "toxic_df_masked = masking(toxic_df, negative)\n",
    "normal_df_masked = masking(normal_df, positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_df_masked[\"label\"] = 1\n",
    "normal_df_masked[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>masked_text</th>\n",
       "      <th>masked_encoded_text</th>\n",
       "      <th>encoded_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just a comment regarding family trusts , they ...</td>\n",
       "      <td>[just, a, comment, regarding, family, trusts, ...</td>\n",
       "      <td>[just, a, comment, regarding, family, trusts, ...</td>\n",
       "      <td>[101, 2074, 1037, 7615, 4953, 2155, 20278, 101...</td>\n",
       "      <td>[101, 2074, 1037, 7615, 4953, 2155, 20278, 101...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nor do they conform to the notion that our tit...</td>\n",
       "      <td>[nor, do, they, conform, to, the, notion, that...</td>\n",
       "      <td>[nor, do, they, conform, to, the, notion, that...</td>\n",
       "      <td>[101, 4496, 2079, 2027, 23758, 2000, 1996, 936...</td>\n",
       "      <td>[101, 4496, 2079, 2027, 23758, 2000, 1996, 936...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yeah , the pers employees and their pensions f...</td>\n",
       "      <td>[yeah, ,, the, per, ##s, employees, and, their...</td>\n",
       "      <td>[yeah, ,, the, [MASK], ##s, [MASK], and, their...</td>\n",
       "      <td>[101, 3398, 1010, 1996, 103, 2015, 103, 1998, ...</td>\n",
       "      <td>[101, 3398, 1010, 1996, 2566, 2015, 5126, 1998...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why risk our marine parks and sea life for a f...</td>\n",
       "      <td>[why, risk, our, marine, parks, and, sea, life...</td>\n",
       "      <td>[why, risk, our, marine, [MASK], and, sea, lif...</td>\n",
       "      <td>[101, 2339, 3891, 2256, 3884, 103, 1998, 2712,...</td>\n",
       "      <td>[101, 2339, 3891, 2256, 3884, 6328, 1998, 2712...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not sure what flavor koolaid you drinking but ...</td>\n",
       "      <td>[not, sure, what, flavor, ko, ##ola, ##id, you...</td>\n",
       "      <td>[not, sure, what, flavor, [MASK], ##ola, ##id,...</td>\n",
       "      <td>[101, 2025, 2469, 2054, 14894, 103, 6030, 3593...</td>\n",
       "      <td>[101, 2025, 2469, 2054, 14894, 12849, 6030, 35...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135385</th>\n",
       "      <td>i think you , other like minded individuals ge...</td>\n",
       "      <td>[i, think, you, ,, other, like, minded, indivi...</td>\n",
       "      <td>[i, think, you, ,, other, like, minded, indivi...</td>\n",
       "      <td>[101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...</td>\n",
       "      <td>[101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135386</th>\n",
       "      <td>a lot of people will be wondering what they we...</td>\n",
       "      <td>[a, lot, of, people, will, be, wondering, what...</td>\n",
       "      <td>[a, lot, of, people, will, be, wondering, what...</td>\n",
       "      <td>[101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...</td>\n",
       "      <td>[101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135387</th>\n",
       "      <td>they have endured saber rattling by the us for...</td>\n",
       "      <td>[they, have, endured, saber, rattling, by, the...</td>\n",
       "      <td>[they, have, endured, saber, rattling, by, the...</td>\n",
       "      <td>[101, 2027, 2031, 16753, 25653, 26347, 2011, 1...</td>\n",
       "      <td>[101, 2027, 2031, 16753, 25653, 26347, 2011, 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135388</th>\n",
       "      <td>the cbc has been totally silent on reporting a...</td>\n",
       "      <td>[the, cbc, has, been, totally, silent, on, rep...</td>\n",
       "      <td>[the, cbc, has, been, totally, silent, on, rep...</td>\n",
       "      <td>[101, 1996, 13581, 2038, 2042, 6135, 4333, 200...</td>\n",
       "      <td>[101, 1996, 13581, 2038, 2042, 6135, 4333, 200...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135389</th>\n",
       "      <td>its not the police fault .\\n</td>\n",
       "      <td>[its, not, the, police, fault, .]</td>\n",
       "      <td>[its, not, the, police, fault, .]</td>\n",
       "      <td>[101, 2049, 2025, 1996, 2610, 6346, 1012, 102]</td>\n",
       "      <td>[101, 2049, 2025, 1996, 2610, 6346, 1012, 102]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135390 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       just a comment regarding family trusts , they ...   \n",
       "1       nor do they conform to the notion that our tit...   \n",
       "2       yeah , the pers employees and their pensions f...   \n",
       "3       why risk our marine parks and sea life for a f...   \n",
       "4       not sure what flavor koolaid you drinking but ...   \n",
       "...                                                   ...   \n",
       "135385  i think you , other like minded individuals ge...   \n",
       "135386  a lot of people will be wondering what they we...   \n",
       "135387  they have endured saber rattling by the us for...   \n",
       "135388  the cbc has been totally silent on reporting a...   \n",
       "135389                       its not the police fault .\\n   \n",
       "\n",
       "                                           tokenized_text  \\\n",
       "0       [just, a, comment, regarding, family, trusts, ...   \n",
       "1       [nor, do, they, conform, to, the, notion, that...   \n",
       "2       [yeah, ,, the, per, ##s, employees, and, their...   \n",
       "3       [why, risk, our, marine, parks, and, sea, life...   \n",
       "4       [not, sure, what, flavor, ko, ##ola, ##id, you...   \n",
       "...                                                   ...   \n",
       "135385  [i, think, you, ,, other, like, minded, indivi...   \n",
       "135386  [a, lot, of, people, will, be, wondering, what...   \n",
       "135387  [they, have, endured, saber, rattling, by, the...   \n",
       "135388  [the, cbc, has, been, totally, silent, on, rep...   \n",
       "135389                  [its, not, the, police, fault, .]   \n",
       "\n",
       "                                              masked_text  \\\n",
       "0       [just, a, comment, regarding, family, trusts, ...   \n",
       "1       [nor, do, they, conform, to, the, notion, that...   \n",
       "2       [yeah, ,, the, [MASK], ##s, [MASK], and, their...   \n",
       "3       [why, risk, our, marine, [MASK], and, sea, lif...   \n",
       "4       [not, sure, what, flavor, [MASK], ##ola, ##id,...   \n",
       "...                                                   ...   \n",
       "135385  [i, think, you, ,, other, like, minded, indivi...   \n",
       "135386  [a, lot, of, people, will, be, wondering, what...   \n",
       "135387  [they, have, endured, saber, rattling, by, the...   \n",
       "135388  [the, cbc, has, been, totally, silent, on, rep...   \n",
       "135389                  [its, not, the, police, fault, .]   \n",
       "\n",
       "                                      masked_encoded_text  \\\n",
       "0       [101, 2074, 1037, 7615, 4953, 2155, 20278, 101...   \n",
       "1       [101, 4496, 2079, 2027, 23758, 2000, 1996, 936...   \n",
       "2       [101, 3398, 1010, 1996, 103, 2015, 103, 1998, ...   \n",
       "3       [101, 2339, 3891, 2256, 3884, 103, 1998, 2712,...   \n",
       "4       [101, 2025, 2469, 2054, 14894, 103, 6030, 3593...   \n",
       "...                                                   ...   \n",
       "135385  [101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...   \n",
       "135386  [101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...   \n",
       "135387  [101, 2027, 2031, 16753, 25653, 26347, 2011, 1...   \n",
       "135388  [101, 1996, 13581, 2038, 2042, 6135, 4333, 200...   \n",
       "135389     [101, 2049, 2025, 1996, 2610, 6346, 1012, 102]   \n",
       "\n",
       "                                             encoded_text  label  \n",
       "0       [101, 2074, 1037, 7615, 4953, 2155, 20278, 101...      0  \n",
       "1       [101, 4496, 2079, 2027, 23758, 2000, 1996, 936...      0  \n",
       "2       [101, 3398, 1010, 1996, 2566, 2015, 5126, 1998...      0  \n",
       "3       [101, 2339, 3891, 2256, 3884, 6328, 1998, 2712...      0  \n",
       "4       [101, 2025, 2469, 2054, 14894, 12849, 6030, 35...      0  \n",
       "...                                                   ...    ...  \n",
       "135385  [101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...      0  \n",
       "135386  [101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...      0  \n",
       "135387  [101, 2027, 2031, 16753, 25653, 26347, 2011, 1...      0  \n",
       "135388  [101, 1996, 13581, 2038, 2042, 6135, 4333, 200...      0  \n",
       "135389     [101, 2049, 2025, 1996, 2610, 6346, 1012, 102]      0  \n",
       "\n",
       "[135390 rows x 6 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_df_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the two dataframes and shuffle them\n",
    "df = pd.concat([toxic_df_masked, normal_df_masked], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "# print avg length of the sentences\n",
    "print(df['masked_encoded_text'].apply(len).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>masked_text</th>\n",
       "      <th>masked_encoded_text</th>\n",
       "      <th>encoded_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuck you , you nixonian twit .\\n</td>\n",
       "      <td>[fuck, you, ,, you, nixon, ##ian, t, ##wi, ##t...</td>\n",
       "      <td>[[MASK], you, ,, you, nixon, ##ian, t, ##wi, #...</td>\n",
       "      <td>[101, 103, 2017, 1010, 2017, 11296, 2937, 1056...</td>\n",
       "      <td>[101, 6616, 2017, 1010, 2017, 11296, 2937, 105...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just another vicious and trashy antitrump , pr...</td>\n",
       "      <td>[just, another, vicious, and, trash, ##y, anti...</td>\n",
       "      <td>[just, another, vicious, and, [MASK], ##y, ant...</td>\n",
       "      <td>[101, 2074, 2178, 13925, 1998, 103, 2100, 3424...</td>\n",
       "      <td>[101, 2074, 2178, 13925, 1998, 11669, 2100, 34...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you want to talk hypocrites , bauer , im up...</td>\n",
       "      <td>[if, you, want, to, talk, h, ##yp, ##oc, ##rit...</td>\n",
       "      <td>[if, you, want, to, talk, h, ##yp, ##oc, ##rit...</td>\n",
       "      <td>[101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...</td>\n",
       "      <td>[101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that you were shocked by the backlash only fur...</td>\n",
       "      <td>[that, you, were, shocked, by, the, backlash, ...</td>\n",
       "      <td>[that, you, were, shocked, by, the, backlash, ...</td>\n",
       "      <td>[101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...</td>\n",
       "      <td>[101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you all suck shapiros cock and lick his mother...</td>\n",
       "      <td>[you, all, suck, shapiro, ##s, cock, and, lick...</td>\n",
       "      <td>[you, all, [MASK], shapiro, ##s, [MASK], and, ...</td>\n",
       "      <td>[101, 2017, 2035, 103, 24630, 2015, 103, 1998,...</td>\n",
       "      <td>[101, 2017, 2035, 11891, 24630, 2015, 10338, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270775</th>\n",
       "      <td>i think you , other like minded individuals ge...</td>\n",
       "      <td>[i, think, you, ,, other, like, minded, indivi...</td>\n",
       "      <td>[i, think, you, ,, other, like, minded, indivi...</td>\n",
       "      <td>[101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...</td>\n",
       "      <td>[101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270776</th>\n",
       "      <td>a lot of people will be wondering what they we...</td>\n",
       "      <td>[a, lot, of, people, will, be, wondering, what...</td>\n",
       "      <td>[a, lot, of, people, will, be, wondering, what...</td>\n",
       "      <td>[101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...</td>\n",
       "      <td>[101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270777</th>\n",
       "      <td>they have endured saber rattling by the us for...</td>\n",
       "      <td>[they, have, endured, saber, rattling, by, the...</td>\n",
       "      <td>[they, have, endured, saber, rattling, by, the...</td>\n",
       "      <td>[101, 2027, 2031, 16753, 25653, 26347, 2011, 1...</td>\n",
       "      <td>[101, 2027, 2031, 16753, 25653, 26347, 2011, 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270778</th>\n",
       "      <td>the cbc has been totally silent on reporting a...</td>\n",
       "      <td>[the, cbc, has, been, totally, silent, on, rep...</td>\n",
       "      <td>[the, cbc, has, been, totally, silent, on, rep...</td>\n",
       "      <td>[101, 1996, 13581, 2038, 2042, 6135, 4333, 200...</td>\n",
       "      <td>[101, 1996, 13581, 2038, 2042, 6135, 4333, 200...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270779</th>\n",
       "      <td>its not the police fault .\\n</td>\n",
       "      <td>[its, not, the, police, fault, .]</td>\n",
       "      <td>[its, not, the, police, fault, .]</td>\n",
       "      <td>[101, 2049, 2025, 1996, 2610, 6346, 1012, 102]</td>\n",
       "      <td>[101, 2049, 2025, 1996, 2610, 6346, 1012, 102]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270780 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0                        fuck you , you nixonian twit .\\n   \n",
       "1       just another vicious and trashy antitrump , pr...   \n",
       "2       if you want to talk hypocrites , bauer , im up...   \n",
       "3       that you were shocked by the backlash only fur...   \n",
       "4       you all suck shapiros cock and lick his mother...   \n",
       "...                                                   ...   \n",
       "270775  i think you , other like minded individuals ge...   \n",
       "270776  a lot of people will be wondering what they we...   \n",
       "270777  they have endured saber rattling by the us for...   \n",
       "270778  the cbc has been totally silent on reporting a...   \n",
       "270779                       its not the police fault .\\n   \n",
       "\n",
       "                                           tokenized_text  \\\n",
       "0       [fuck, you, ,, you, nixon, ##ian, t, ##wi, ##t...   \n",
       "1       [just, another, vicious, and, trash, ##y, anti...   \n",
       "2       [if, you, want, to, talk, h, ##yp, ##oc, ##rit...   \n",
       "3       [that, you, were, shocked, by, the, backlash, ...   \n",
       "4       [you, all, suck, shapiro, ##s, cock, and, lick...   \n",
       "...                                                   ...   \n",
       "270775  [i, think, you, ,, other, like, minded, indivi...   \n",
       "270776  [a, lot, of, people, will, be, wondering, what...   \n",
       "270777  [they, have, endured, saber, rattling, by, the...   \n",
       "270778  [the, cbc, has, been, totally, silent, on, rep...   \n",
       "270779                  [its, not, the, police, fault, .]   \n",
       "\n",
       "                                              masked_text  \\\n",
       "0       [[MASK], you, ,, you, nixon, ##ian, t, ##wi, #...   \n",
       "1       [just, another, vicious, and, [MASK], ##y, ant...   \n",
       "2       [if, you, want, to, talk, h, ##yp, ##oc, ##rit...   \n",
       "3       [that, you, were, shocked, by, the, backlash, ...   \n",
       "4       [you, all, [MASK], shapiro, ##s, [MASK], and, ...   \n",
       "...                                                   ...   \n",
       "270775  [i, think, you, ,, other, like, minded, indivi...   \n",
       "270776  [a, lot, of, people, will, be, wondering, what...   \n",
       "270777  [they, have, endured, saber, rattling, by, the...   \n",
       "270778  [the, cbc, has, been, totally, silent, on, rep...   \n",
       "270779                  [its, not, the, police, fault, .]   \n",
       "\n",
       "                                      masked_encoded_text  \\\n",
       "0       [101, 103, 2017, 1010, 2017, 11296, 2937, 1056...   \n",
       "1       [101, 2074, 2178, 13925, 1998, 103, 2100, 3424...   \n",
       "2       [101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...   \n",
       "3       [101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...   \n",
       "4       [101, 2017, 2035, 103, 24630, 2015, 103, 1998,...   \n",
       "...                                                   ...   \n",
       "270775  [101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...   \n",
       "270776  [101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...   \n",
       "270777  [101, 2027, 2031, 16753, 25653, 26347, 2011, 1...   \n",
       "270778  [101, 1996, 13581, 2038, 2042, 6135, 4333, 200...   \n",
       "270779     [101, 2049, 2025, 1996, 2610, 6346, 1012, 102]   \n",
       "\n",
       "                                             encoded_text  label  \n",
       "0       [101, 6616, 2017, 1010, 2017, 11296, 2937, 105...      1  \n",
       "1       [101, 2074, 2178, 13925, 1998, 11669, 2100, 34...      1  \n",
       "2       [101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...      1  \n",
       "3       [101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...      1  \n",
       "4       [101, 2017, 2035, 11891, 24630, 2015, 10338, 1...      1  \n",
       "...                                                   ...    ...  \n",
       "270775  [101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...      0  \n",
       "270776  [101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...      0  \n",
       "270777  [101, 2027, 2031, 16753, 25653, 26347, 2011, 1...      0  \n",
       "270778  [101, 1996, 13581, 2038, 2042, 6135, 4333, 200...      0  \n",
       "270779     [101, 2049, 2025, 1996, 2610, 6346, 1012, 102]      0  \n",
       "\n",
       "[270780 rows x 6 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe in pickle file\n",
    "df.to_pickle(\"../data/data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from pickle file\n",
    "df = pd.read_pickle(\"../data/data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>masked_text</th>\n",
       "      <th>masked_encoded_text</th>\n",
       "      <th>encoded_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuck you , you nixonian twit .\\n</td>\n",
       "      <td>[fuck, you, ,, you, nixon, ##ian, t, ##wi, ##t...</td>\n",
       "      <td>[[MASK], you, ,, you, nixon, ##ian, t, ##wi, #...</td>\n",
       "      <td>[101, 103, 2017, 1010, 2017, 11296, 2937, 1056...</td>\n",
       "      <td>[101, 6616, 2017, 1010, 2017, 11296, 2937, 105...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just another vicious and trashy antitrump , pr...</td>\n",
       "      <td>[just, another, vicious, and, trash, ##y, anti...</td>\n",
       "      <td>[just, another, vicious, and, [MASK], ##y, ant...</td>\n",
       "      <td>[101, 2074, 2178, 13925, 1998, 103, 2100, 3424...</td>\n",
       "      <td>[101, 2074, 2178, 13925, 1998, 11669, 2100, 34...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you want to talk hypocrites , bauer , im up...</td>\n",
       "      <td>[if, you, want, to, talk, h, ##yp, ##oc, ##rit...</td>\n",
       "      <td>[if, you, want, to, talk, h, ##yp, ##oc, ##rit...</td>\n",
       "      <td>[101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...</td>\n",
       "      <td>[101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that you were shocked by the backlash only fur...</td>\n",
       "      <td>[that, you, were, shocked, by, the, backlash, ...</td>\n",
       "      <td>[that, you, were, shocked, by, the, backlash, ...</td>\n",
       "      <td>[101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...</td>\n",
       "      <td>[101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you all suck shapiros cock and lick his mother...</td>\n",
       "      <td>[you, all, suck, shapiro, ##s, cock, and, lick...</td>\n",
       "      <td>[you, all, [MASK], shapiro, ##s, [MASK], and, ...</td>\n",
       "      <td>[101, 2017, 2035, 103, 24630, 2015, 103, 1998,...</td>\n",
       "      <td>[101, 2017, 2035, 11891, 24630, 2015, 10338, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270775</th>\n",
       "      <td>i think you , other like minded individuals ge...</td>\n",
       "      <td>[i, think, you, ,, other, like, minded, indivi...</td>\n",
       "      <td>[i, think, you, ,, other, like, minded, indivi...</td>\n",
       "      <td>[101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...</td>\n",
       "      <td>[101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270776</th>\n",
       "      <td>a lot of people will be wondering what they we...</td>\n",
       "      <td>[a, lot, of, people, will, be, wondering, what...</td>\n",
       "      <td>[a, lot, of, people, will, be, wondering, what...</td>\n",
       "      <td>[101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...</td>\n",
       "      <td>[101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270777</th>\n",
       "      <td>they have endured saber rattling by the us for...</td>\n",
       "      <td>[they, have, endured, saber, rattling, by, the...</td>\n",
       "      <td>[they, have, endured, saber, rattling, by, the...</td>\n",
       "      <td>[101, 2027, 2031, 16753, 25653, 26347, 2011, 1...</td>\n",
       "      <td>[101, 2027, 2031, 16753, 25653, 26347, 2011, 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270778</th>\n",
       "      <td>the cbc has been totally silent on reporting a...</td>\n",
       "      <td>[the, cbc, has, been, totally, silent, on, rep...</td>\n",
       "      <td>[the, cbc, has, been, totally, silent, on, rep...</td>\n",
       "      <td>[101, 1996, 13581, 2038, 2042, 6135, 4333, 200...</td>\n",
       "      <td>[101, 1996, 13581, 2038, 2042, 6135, 4333, 200...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270779</th>\n",
       "      <td>its not the police fault .\\n</td>\n",
       "      <td>[its, not, the, police, fault, .]</td>\n",
       "      <td>[its, not, the, police, fault, .]</td>\n",
       "      <td>[101, 2049, 2025, 1996, 2610, 6346, 1012, 102]</td>\n",
       "      <td>[101, 2049, 2025, 1996, 2610, 6346, 1012, 102]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270780 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0                        fuck you , you nixonian twit .\\n   \n",
       "1       just another vicious and trashy antitrump , pr...   \n",
       "2       if you want to talk hypocrites , bauer , im up...   \n",
       "3       that you were shocked by the backlash only fur...   \n",
       "4       you all suck shapiros cock and lick his mother...   \n",
       "...                                                   ...   \n",
       "270775  i think you , other like minded individuals ge...   \n",
       "270776  a lot of people will be wondering what they we...   \n",
       "270777  they have endured saber rattling by the us for...   \n",
       "270778  the cbc has been totally silent on reporting a...   \n",
       "270779                       its not the police fault .\\n   \n",
       "\n",
       "                                           tokenized_text  \\\n",
       "0       [fuck, you, ,, you, nixon, ##ian, t, ##wi, ##t...   \n",
       "1       [just, another, vicious, and, trash, ##y, anti...   \n",
       "2       [if, you, want, to, talk, h, ##yp, ##oc, ##rit...   \n",
       "3       [that, you, were, shocked, by, the, backlash, ...   \n",
       "4       [you, all, suck, shapiro, ##s, cock, and, lick...   \n",
       "...                                                   ...   \n",
       "270775  [i, think, you, ,, other, like, minded, indivi...   \n",
       "270776  [a, lot, of, people, will, be, wondering, what...   \n",
       "270777  [they, have, endured, saber, rattling, by, the...   \n",
       "270778  [the, cbc, has, been, totally, silent, on, rep...   \n",
       "270779                  [its, not, the, police, fault, .]   \n",
       "\n",
       "                                              masked_text  \\\n",
       "0       [[MASK], you, ,, you, nixon, ##ian, t, ##wi, #...   \n",
       "1       [just, another, vicious, and, [MASK], ##y, ant...   \n",
       "2       [if, you, want, to, talk, h, ##yp, ##oc, ##rit...   \n",
       "3       [that, you, were, shocked, by, the, backlash, ...   \n",
       "4       [you, all, [MASK], shapiro, ##s, [MASK], and, ...   \n",
       "...                                                   ...   \n",
       "270775  [i, think, you, ,, other, like, minded, indivi...   \n",
       "270776  [a, lot, of, people, will, be, wondering, what...   \n",
       "270777  [they, have, endured, saber, rattling, by, the...   \n",
       "270778  [the, cbc, has, been, totally, silent, on, rep...   \n",
       "270779                  [its, not, the, police, fault, .]   \n",
       "\n",
       "                                      masked_encoded_text  \\\n",
       "0       [101, 103, 2017, 1010, 2017, 11296, 2937, 1056...   \n",
       "1       [101, 2074, 2178, 13925, 1998, 103, 2100, 3424...   \n",
       "2       [101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...   \n",
       "3       [101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...   \n",
       "4       [101, 2017, 2035, 103, 24630, 2015, 103, 1998,...   \n",
       "...                                                   ...   \n",
       "270775  [101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...   \n",
       "270776  [101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...   \n",
       "270777  [101, 2027, 2031, 16753, 25653, 26347, 2011, 1...   \n",
       "270778  [101, 1996, 13581, 2038, 2042, 6135, 4333, 200...   \n",
       "270779     [101, 2049, 2025, 1996, 2610, 6346, 1012, 102]   \n",
       "\n",
       "                                             encoded_text  label  \n",
       "0       [101, 6616, 2017, 1010, 2017, 11296, 2937, 105...      1  \n",
       "1       [101, 2074, 2178, 13925, 1998, 11669, 2100, 34...      1  \n",
       "2       [101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...      1  \n",
       "3       [101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...      1  \n",
       "4       [101, 2017, 2035, 11891, 24630, 2015, 10338, 1...      1  \n",
       "...                                                   ...    ...  \n",
       "270775  [101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...      0  \n",
       "270776  [101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...      0  \n",
       "270777  [101, 2027, 2031, 16753, 25653, 26347, 2011, 1...      0  \n",
       "270778  [101, 1996, 13581, 2038, 2042, 6135, 4333, 200...      0  \n",
       "270779     [101, 2049, 2025, 1996, 2610, 6346, 1012, 102]      0  \n",
       "\n",
       "[270780 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, val and test\n",
    "train, val, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTFineTuneDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,df, maxlen = 64):\n",
    "    super(BERTFineTuneDataset, self).__init__()\n",
    "    # pad the encoded text to maxlen\n",
    "    self._encoded_text = torch.nn.utils.rnn.pad_sequence(df['encoded_text'].apply(torch.tensor), batch_first=True, padding_value=0)\n",
    "    df[\"labels\"] = df[\"label\"].apply((lambda x: torch.ones(maxlen).int() if x == 1 else torch.zeros(maxlen).int()))\n",
    "    \n",
    "    self._labels = df['labels'].values\n",
    "\n",
    "    self._maxlen = maxlen\n",
    "  \n",
    "    self._masked_encoded_text = torch.nn.utils.rnn.pad_sequence(df['masked_encoded_text'].apply(torch.tensor), batch_first=True, padding_value=0)\n",
    "    self._attention_mask = torch.nn.utils.rnn.pad_sequence(df['masked_encoded_text'].apply(lambda x: torch.tensor([1 if i != 0 else 0 for i in x])), batch_first=True, padding_value=0)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self._labels)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    # make dict of the data\n",
    "    return {\n",
    "\n",
    "        'encoded_text': self._encoded_text[idx][:self._maxlen],\n",
    "        'masked_encoded_text': self._masked_encoded_text[idx][:self._maxlen],\n",
    "        'attention_mask': self._attention_mask[idx][:self._maxlen],\n",
    "        'labels': self._labels[idx][:self._maxlen]    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "train_dataset = BERTFineTuneDataset(train)\n",
    "val_dataset = BERTFineTuneDataset(val)\n",
    "test_dataset = BERTFineTuneDataset(test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataloader in pickle file\n",
    "with open(\"../data/train_dataloader.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_dataloader, f)\n",
    "\n",
    "with open(\"../data/val_dataloader.pkl\", 'wb') as f:\n",
    "    pickle.dump(val_dataloader, f)\n",
    "\n",
    "with open(\"../data/test_dataloader.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_dataloader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'BERTFineTuneDataset' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# get the dataloader from pickle file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m./data/train_dataloader.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     train_dataloader \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'BERTFineTuneDataset' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "# get the dataloader from pickle file\n",
    "with open(\"../data/train_dataloader.pkl\", 'rb') as f:\n",
    "    train_dataloader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[\"labels\"])\n",
    "    print(len(batch[\"labels\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home2/esh/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 5078/5078 [13:38<00:00,  6.21it/s, loss=0.0794]\n",
      "Epoch 1: 100%|██████████| 5078/5078 [13:37<00:00,  6.21it/s, loss=0.0355] \n",
      "Epoch 2: 100%|██████████| 5078/5078 [13:38<00:00,  6.20it/s, loss=7.31e-5]\n",
      "Epoch 3:   3%|▎         | 134/5078 [00:21<13:15,  6.21it/s, loss=0.0346]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [283], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     31\u001b[0m loop\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m loop\u001b[39m.\u001b[39mset_postfix(loss\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:362\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    360\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m(\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta1))\n\u001b[1;32m    361\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m--> 362\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39;49msqrt()\u001b[39m.\u001b[39;49madd_(group[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    364\u001b[0m step_size \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    365\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m\"\u001b[39m\u001b[39mcorrect_bias\u001b[39m\u001b[39m\"\u001b[39m]:  \u001b[39m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#fine tuning bert model\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model.to(torch.device('cuda'))\n",
    "\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(10):\n",
    "    loop  = tqdm(train_dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['masked_encoded_text'].to(torch.device('cuda'))\n",
    "        attention_mask = batch['attention_mask'].to(torch.device('cuda'))\n",
    "        labels = batch['encoded_text'].to(torch.device('cuda'))\n",
    "        segment_ids = batch['labels'].to(torch.device('cuda'))\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,token_type_ids = segment_ids, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home2/esh/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 5078/5078 [14:27<00:00,  5.85it/s, loss=0.0174]\n",
      "Epoch 1: 100%|██████████| 5078/5078 [14:34<00:00,  5.81it/s, loss=0.0618] \n",
      "Epoch 2: 100%|██████████| 5078/5078 [14:23<00:00,  5.88it/s, loss=0.0265] \n",
      "Epoch 3: 100%|██████████| 5078/5078 [14:13<00:00,  5.95it/s, loss=0.0249] \n",
      "Epoch 4: 100%|██████████| 5078/5078 [14:13<00:00,  5.95it/s, loss=0.00897]\n"
     ]
    }
   ],
   "source": [
    "#fine tuning bert model\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model.to(torch.device('cuda'))\n",
    "\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop  = tqdm(train_dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['masked_encoded_text'].to(torch.device('cuda'))\n",
    "        attention_mask = batch['attention_mask'].to(torch.device('cuda'))\n",
    "        labels = batch['encoded_text'].to(torch.device('cuda'))\n",
    "        segment_ids = batch['labels'].to(torch.device('cuda'))\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,token_type_ids = segment_ids, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    path = \"../models/bert_detox_ft_\"+str(epoch+1)+\"epochs.pth\"\n",
    "    torch.save(model, path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the finetuned model\n",
    "model.save_pretrained(\"../model/BERT_finetuned3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../model/bert_detox_ft_3epochs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation code\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "for batch in val_dataloader:\n",
    "    input_ids = batch['masked_encoded_text'].to(torch.device('cuda'))\n",
    "    attention_mask = batch['attention_mask'].to(torch.device('cuda'))\n",
    "    labels = batch['encoded_text'].to(torch.device('cuda'))\n",
    "    segment_ids = batch['labels'].to(torch.device('cuda'))\n",
    "    outputs = model(input_ids, attention_mask=attention_mask,token_type_ids = segment_ids, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    val_loss += loss.item()\n",
    "\n",
    "print(val_loss/len(val_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "for batch in test_dataloader:\n",
    "    input_ids = batch['masked_encoded_text'].to(torch.device('cuda'))\n",
    "    attention_mask = batch['attention_mask'].to(torch.device('cuda'))\n",
    "    labels = batch['encoded_text'].to(torch.device('cuda'))\n",
    "    segment_ids = batch['labels'].to(torch.device('cuda'))\n",
    "    outputs = model(input_ids, attention_mask=attention_mask,token_type_ids = segment_ids, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    test_loss += loss.item()\n",
    "\n",
    "print(test_loss/len(test_dataloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask one word at random - Fine Tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# df[\"masked_random_encoding\"] = df[\"tokenized_text\"].apply(lambda x: [ i[random.randint(1, len(i)-1)] = 103 for i in x]) # 103 is the encoding for [MASK]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# make a new column for masked random encoding\n",
    "masked_random_encoding = []\n",
    "masked_random = []\n",
    "\n",
    "for i in df[\"tokenized_text\"]:\n",
    "    temp = i.copy()\n",
    "    rand = random.randint(1, len(i)-1)\n",
    "    temp[rand] = \"[MASK]\"\n",
    "    masked_random.append(temp)\n",
    "\n",
    "    masked_random_encoding.append(tokenizer.encode(temp))\n",
    "\n",
    "df[\"masked_random_encoding\"] = masked_random_encoding\n",
    "df[\"masked_random\"] = masked_random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>masked_text</th>\n",
       "      <th>masked_encoded_text</th>\n",
       "      <th>encoded_text</th>\n",
       "      <th>label</th>\n",
       "      <th>masked_random_encoding</th>\n",
       "      <th>masked_random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuck you , you nixonian twit .\\n</td>\n",
       "      <td>[fuck, you, ,, you, nixon, ##ian, t, ##wi, ##t...</td>\n",
       "      <td>[[MASK], you, ,, you, nixon, ##ian, t, ##wi, #...</td>\n",
       "      <td>[101, 103, 2017, 1010, 2017, 11296, 2937, 1056...</td>\n",
       "      <td>[101, 6616, 2017, 1010, 2017, 11296, 2937, 105...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 6616, 103, 1010, 2017, 11296, 2937, 1056...</td>\n",
       "      <td>[fuck, [MASK], ,, you, nixon, ##ian, t, ##wi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just another vicious and trashy antitrump , pr...</td>\n",
       "      <td>[just, another, vicious, and, trash, ##y, anti...</td>\n",
       "      <td>[just, another, vicious, and, [MASK], ##y, ant...</td>\n",
       "      <td>[101, 2074, 2178, 13925, 1998, 103, 2100, 3424...</td>\n",
       "      <td>[101, 2074, 2178, 13925, 1998, 11669, 2100, 34...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2074, 2178, 13925, 1998, 11669, 2100, 34...</td>\n",
       "      <td>[just, another, vicious, and, trash, ##y, anti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you want to talk hypocrites , bauer , im up...</td>\n",
       "      <td>[if, you, want, to, talk, h, ##yp, ##oc, ##rit...</td>\n",
       "      <td>[if, you, want, to, talk, h, ##yp, ##oc, ##rit...</td>\n",
       "      <td>[101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...</td>\n",
       "      <td>[101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...</td>\n",
       "      <td>[if, you, want, to, talk, h, ##yp, ##oc, [MASK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that you were shocked by the backlash only fur...</td>\n",
       "      <td>[that, you, were, shocked, by, the, backlash, ...</td>\n",
       "      <td>[that, you, were, shocked, by, the, backlash, ...</td>\n",
       "      <td>[101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...</td>\n",
       "      <td>[101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...</td>\n",
       "      <td>[that, you, were, shocked, by, the, backlash, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you all suck shapiros cock and lick his mother...</td>\n",
       "      <td>[you, all, suck, shapiro, ##s, cock, and, lick...</td>\n",
       "      <td>[you, all, [MASK], shapiro, ##s, [MASK], and, ...</td>\n",
       "      <td>[101, 2017, 2035, 103, 24630, 2015, 103, 1998,...</td>\n",
       "      <td>[101, 2017, 2035, 11891, 24630, 2015, 10338, 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2017, 2035, 11891, 24630, 103, 10338, 19...</td>\n",
       "      <td>[you, all, suck, shapiro, [MASK], cock, and, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270775</th>\n",
       "      <td>i think you , other like minded individuals ge...</td>\n",
       "      <td>[i, think, you, ,, other, like, minded, indivi...</td>\n",
       "      <td>[i, think, you, ,, other, like, minded, indivi...</td>\n",
       "      <td>[101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...</td>\n",
       "      <td>[101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...</td>\n",
       "      <td>[i, think, you, ,, other, like, minded, indivi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270776</th>\n",
       "      <td>a lot of people will be wondering what they we...</td>\n",
       "      <td>[a, lot, of, people, will, be, wondering, what...</td>\n",
       "      <td>[a, lot, of, people, will, be, wondering, what...</td>\n",
       "      <td>[101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...</td>\n",
       "      <td>[101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...</td>\n",
       "      <td>[a, lot, of, people, will, be, wondering, what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270777</th>\n",
       "      <td>they have endured saber rattling by the us for...</td>\n",
       "      <td>[they, have, endured, saber, rattling, by, the...</td>\n",
       "      <td>[they, have, endured, saber, rattling, by, the...</td>\n",
       "      <td>[101, 2027, 2031, 16753, 25653, 26347, 2011, 1...</td>\n",
       "      <td>[101, 2027, 2031, 16753, 25653, 26347, 2011, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 2027, 2031, 16753, 25653, 26347, 2011, 1...</td>\n",
       "      <td>[they, have, endured, saber, rattling, by, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270778</th>\n",
       "      <td>the cbc has been totally silent on reporting a...</td>\n",
       "      <td>[the, cbc, has, been, totally, silent, on, rep...</td>\n",
       "      <td>[the, cbc, has, been, totally, silent, on, rep...</td>\n",
       "      <td>[101, 1996, 13581, 2038, 2042, 6135, 4333, 200...</td>\n",
       "      <td>[101, 1996, 13581, 2038, 2042, 6135, 4333, 200...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1996, 13581, 2038, 2042, 6135, 103, 2006...</td>\n",
       "      <td>[the, cbc, has, been, totally, [MASK], on, rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270779</th>\n",
       "      <td>its not the police fault .\\n</td>\n",
       "      <td>[its, not, the, police, fault, .]</td>\n",
       "      <td>[its, not, the, police, fault, .]</td>\n",
       "      <td>[101, 2049, 2025, 1996, 2610, 6346, 1012, 102]</td>\n",
       "      <td>[101, 2049, 2025, 1996, 2610, 6346, 1012, 102]</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 2049, 2025, 1996, 2610, 6346, 103, 102]</td>\n",
       "      <td>[its, not, the, police, fault, [MASK]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270780 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0                        fuck you , you nixonian twit .\\n   \n",
       "1       just another vicious and trashy antitrump , pr...   \n",
       "2       if you want to talk hypocrites , bauer , im up...   \n",
       "3       that you were shocked by the backlash only fur...   \n",
       "4       you all suck shapiros cock and lick his mother...   \n",
       "...                                                   ...   \n",
       "270775  i think you , other like minded individuals ge...   \n",
       "270776  a lot of people will be wondering what they we...   \n",
       "270777  they have endured saber rattling by the us for...   \n",
       "270778  the cbc has been totally silent on reporting a...   \n",
       "270779                       its not the police fault .\\n   \n",
       "\n",
       "                                           tokenized_text  \\\n",
       "0       [fuck, you, ,, you, nixon, ##ian, t, ##wi, ##t...   \n",
       "1       [just, another, vicious, and, trash, ##y, anti...   \n",
       "2       [if, you, want, to, talk, h, ##yp, ##oc, ##rit...   \n",
       "3       [that, you, were, shocked, by, the, backlash, ...   \n",
       "4       [you, all, suck, shapiro, ##s, cock, and, lick...   \n",
       "...                                                   ...   \n",
       "270775  [i, think, you, ,, other, like, minded, indivi...   \n",
       "270776  [a, lot, of, people, will, be, wondering, what...   \n",
       "270777  [they, have, endured, saber, rattling, by, the...   \n",
       "270778  [the, cbc, has, been, totally, silent, on, rep...   \n",
       "270779                  [its, not, the, police, fault, .]   \n",
       "\n",
       "                                              masked_text  \\\n",
       "0       [[MASK], you, ,, you, nixon, ##ian, t, ##wi, #...   \n",
       "1       [just, another, vicious, and, [MASK], ##y, ant...   \n",
       "2       [if, you, want, to, talk, h, ##yp, ##oc, ##rit...   \n",
       "3       [that, you, were, shocked, by, the, backlash, ...   \n",
       "4       [you, all, [MASK], shapiro, ##s, [MASK], and, ...   \n",
       "...                                                   ...   \n",
       "270775  [i, think, you, ,, other, like, minded, indivi...   \n",
       "270776  [a, lot, of, people, will, be, wondering, what...   \n",
       "270777  [they, have, endured, saber, rattling, by, the...   \n",
       "270778  [the, cbc, has, been, totally, silent, on, rep...   \n",
       "270779                  [its, not, the, police, fault, .]   \n",
       "\n",
       "                                      masked_encoded_text  \\\n",
       "0       [101, 103, 2017, 1010, 2017, 11296, 2937, 1056...   \n",
       "1       [101, 2074, 2178, 13925, 1998, 103, 2100, 3424...   \n",
       "2       [101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...   \n",
       "3       [101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...   \n",
       "4       [101, 2017, 2035, 103, 24630, 2015, 103, 1998,...   \n",
       "...                                                   ...   \n",
       "270775  [101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...   \n",
       "270776  [101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...   \n",
       "270777  [101, 2027, 2031, 16753, 25653, 26347, 2011, 1...   \n",
       "270778  [101, 1996, 13581, 2038, 2042, 6135, 4333, 200...   \n",
       "270779     [101, 2049, 2025, 1996, 2610, 6346, 1012, 102]   \n",
       "\n",
       "                                             encoded_text  label  \\\n",
       "0       [101, 6616, 2017, 1010, 2017, 11296, 2937, 105...      1   \n",
       "1       [101, 2074, 2178, 13925, 1998, 11669, 2100, 34...      1   \n",
       "2       [101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...      1   \n",
       "3       [101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...      1   \n",
       "4       [101, 2017, 2035, 11891, 24630, 2015, 10338, 1...      1   \n",
       "...                                                   ...    ...   \n",
       "270775  [101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...      0   \n",
       "270776  [101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...      0   \n",
       "270777  [101, 2027, 2031, 16753, 25653, 26347, 2011, 1...      0   \n",
       "270778  [101, 1996, 13581, 2038, 2042, 6135, 4333, 200...      0   \n",
       "270779     [101, 2049, 2025, 1996, 2610, 6346, 1012, 102]      0   \n",
       "\n",
       "                                   masked_random_encoding  \\\n",
       "0       [101, 6616, 103, 1010, 2017, 11296, 2937, 1056...   \n",
       "1       [101, 2074, 2178, 13925, 1998, 11669, 2100, 34...   \n",
       "2       [101, 2065, 2017, 2215, 2000, 2831, 1044, 2257...   \n",
       "3       [101, 2008, 2017, 2020, 7135, 2011, 1996, 2574...   \n",
       "4       [101, 2017, 2035, 11891, 24630, 103, 10338, 19...   \n",
       "...                                                   ...   \n",
       "270775  [101, 1045, 2228, 2017, 1010, 2060, 2066, 1312...   \n",
       "270776  [101, 1037, 2843, 1997, 2111, 2097, 2022, 6603...   \n",
       "270777  [101, 2027, 2031, 16753, 25653, 26347, 2011, 1...   \n",
       "270778  [101, 1996, 13581, 2038, 2042, 6135, 103, 2006...   \n",
       "270779      [101, 2049, 2025, 1996, 2610, 6346, 103, 102]   \n",
       "\n",
       "                                            masked_random  \n",
       "0       [fuck, [MASK], ,, you, nixon, ##ian, t, ##wi, ...  \n",
       "1       [just, another, vicious, and, trash, ##y, anti...  \n",
       "2       [if, you, want, to, talk, h, ##yp, ##oc, [MASK...  \n",
       "3       [that, you, were, shocked, by, the, backlash, ...  \n",
       "4       [you, all, suck, shapiro, [MASK], cock, and, l...  \n",
       "...                                                   ...  \n",
       "270775  [i, think, you, ,, other, like, minded, indivi...  \n",
       "270776  [a, lot, of, people, will, be, wondering, what...  \n",
       "270777  [they, have, endured, saber, rattling, by, the...  \n",
       "270778  [the, cbc, has, been, totally, [MASK], on, rep...  \n",
       "270779             [its, not, the, police, fault, [MASK]]  \n",
       "\n",
       "[270780 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, val and test\n",
    "train_rand, val_rand, test_rand = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "# save the data in pickle file\n",
    "with open(\"../data/train_rand.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_rand, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTFineTuneRandDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,df, maxlen = 64):\n",
    "    super(BERTFineTuneRandDataset, self).__init__()\n",
    "    # pad the encoded text to maxlen\n",
    "    self._encoded_text = torch.nn.utils.rnn.pad_sequence(df['encoded_text'].apply(torch.tensor), batch_first=True, padding_value=0)\n",
    "    df[\"labels\"] = df[\"label\"].apply((lambda x: torch.ones(maxlen).int() if x == 1 else torch.zeros(maxlen).int()))\n",
    "    \n",
    "    self._labels = df['labels'].values\n",
    "\n",
    "    self._maxlen = maxlen\n",
    "  \n",
    "    self._masked_encoded_text = torch.nn.utils.rnn.pad_sequence(df['masked_random_encoding'].apply(torch.tensor), batch_first=True, padding_value=0)\n",
    "    self._attention_mask = torch.nn.utils.rnn.pad_sequence(df['masked_random_encoding'].apply(lambda x: torch.tensor([1 if i != 0 else 0 for i in x])), batch_first=True, padding_value=0)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self._labels)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    # make dict of the data\n",
    "    return {\n",
    "\n",
    "        'encoded_text': self._encoded_text[idx][:self._maxlen],\n",
    "        'masked_random_encoding': self._masked_encoded_text[idx][:self._maxlen],\n",
    "        'attention_mask': self._attention_mask[idx][:self._maxlen],\n",
    "        'labels': self._labels[idx][:self._maxlen]    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "train_dataset_rand = BERTFineTuneDataset(train_rand)\n",
    "val_dataset_rand = BERTFineTuneDataset(val_rand)\n",
    "test_dataset_rand = BERTFineTuneDataset(test_rand)\n",
    "\n",
    "train_dataloader_rand = torch.utils.data.DataLoader(train_dataset_rand, batch_size=32, shuffle=True)\n",
    "val_dataloader_rand = torch.utils.data.DataLoader(val_dataset_rand, batch_size=32, shuffle=True)\n",
    "test_dataloader_rand = torch.utils.data.DataLoader(test_dataset_rand, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataloader in pickle file\n",
    "with open(\"../data/train_dataloader_rand.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_dataloader_rand, f)\n",
    "\n",
    "with open(\"../data/val_dataloader_rand.pkl\", 'wb') as f:\n",
    "    pickle.dump(val_dataloader_rand, f)\n",
    "\n",
    "with open(\"../data/test_dataloader_rand.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_dataloader_rand, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning bert model\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model.to(torch.device('cuda'))\n",
    "\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop  = tqdm(train_dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['masked_encoded_text'].to(torch.device('cuda'))\n",
    "        attention_mask = batch['attention_mask'].to(torch.device('cuda'))\n",
    "        labels = batch['encoded_text'].to(torch.device('cuda'))\n",
    "        segment_ids = batch['labels'].to(torch.device('cuda'))\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,token_type_ids = segment_ids, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    path = \"./model/bert_detox_rand_ft_\"+str(epoch+1)+\"epochs.pth\"\n",
    "    torch.save(model, path)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db9d433ab76aa2bebc7886b62d36feb1292e7830d52ec4e11c27f277e0aea8dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
