{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk import ngrams\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fe54ba86664c148d3f56a472f03d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164b709189464273964a152e2cb30b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab38a819ae9f4ff29a1bc2ac0e50a467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a67ee41251421697c8ea062d99167a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the toxic word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_word_path='../data/vocab/negative_words.txt'\n",
    "\n",
    "with open(toxic_word_path, 'r') as f:\n",
    "    toxic_word_list = f.read().splitlines()\n",
    "toxic_word_list = [line.strip() for line in toxic_word_list]\n",
    "\n",
    "def bag_of_words(sentence, list=toxic_word_list):\n",
    "\n",
    "    sentence_list = sentence.split()\n",
    "\n",
    "    for word in sentence_list:\n",
    "        if word in list:\n",
    "            sentence = sentence.replace(word, \"[MASK]\")\n",
    "\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I [MASK] hate you'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words('I fucking hate you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = corpus_tox + corpus_norm\n",
    "y_train = [1] * len(corpus_tox) + [0] * len(corpus_norm)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pipe[1].coef_[0]\n",
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2coef = {w: coefs[idx] for w, idx in pipe[0].vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(VOCAB_DIRNAME + '/word2coef.pkl', 'wb') as f:\n",
    "    pickle.dump(word2coef, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the fine-tuned BERT to predict alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model: Random mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model: Targetted mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking based on Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ranker(similarity_scorer, toxicity_scorer, fluency_scorer, possible_sentences):\n",
    "\n",
    "  max_score = -1\n",
    "\n",
    "  detoxed_sentence = ''\n",
    "\n",
    "  for sentence in possible_sentences:\n",
    "    \n",
    "    sent_score = similarity_scorer(sentence)*toxicity_scorer(sentence)*fluency_scorer(sentence)\n",
    "\n",
    "    if sent_score > max_score:\n",
    "      detoxed_sentence = sentence\n",
    "    \n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(masker, predictor, similarity_scorer, toxicity_scorer, fluency_scorer, ranker, input_sentence, model):\n",
    "\n",
    "  # input_sentence is the sentence we wish to detoxify\n",
    "  # masker inputs sentence as string, outputs sentence as string with [MASK] tokens\n",
    "  # predictor inputs sentence with [MASK], returns a list of N sentences, with the masks replaced\n",
    "  # similarity _scorer inputs 2 sentences and returns a value, how similar they are\n",
    "  # toxicity_scorer inputs 1 sentence and returns a value, how toxic the sentence is\n",
    "  # fluency_scorer, inputs 1 sentence and returns a value, how fluent the sentence is\n",
    "  # ranker inputs a list of N sentences, and returns one sentence, the most appropriately detoxed sentence\n",
    "  # model is the fine-tuned model used for the prediction task\n",
    "\n",
    "\n",
    "  masked_sentence = masker(input_sentence)\n",
    "\n",
    "  possible_sentences = predictor(masked_sentence, N, model)\n",
    "\n",
    "  detoxed_sentence = ranker(similarity_scorer,toxicity_scorer,fluency_scorer, possible_sentences)\n",
    "\n",
    "  return detoxed_sentence\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db9d433ab76aa2bebc7886b62d36feb1292e7830d52ec4e11c27f277e0aea8dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
