{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8ZqAORbeKVi",
        "outputId": "7e9136d9-222d-4593-eb65-635cea3692b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdaU62zZeHn2",
        "outputId": "d37f3e2c-d49b-431b-bfed-be9aab985833"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM, RobertaTokenizer, RobertaForSequenceClassification, GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from nltk import ngrams\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "sw = stopwords.words('english') \n",
        "\n",
        "from collections import Counter\n",
        "c = Counter()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNAUzDoeeHn4",
        "outputId": "1fb84613-5632-4d36-ebe0-292549beb378"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "tokenizer_roberta = RobertaTokenizer.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
        "model_roberta = RobertaForSequenceClassification.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
        "\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "with torch.no_grad():\n",
        "    model_gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    model_gpt.eval()\n",
        "\n",
        "model_random = torch.load('/content/drive/MyDrive/ANLP-project/bert_detox_rand_ft_10epochs.pth',map_location=torch.device('cpu'))\n",
        "model_targeted = torch.load('/content/drive/MyDrive/ANLP-project/bert_detox_ft_5epochs.pth',map_location=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7fdmL9CeagY",
        "outputId": "d72e440e-a92b-4c31-97c5-f382802369ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_erCq9meHn5"
      },
      "source": [
        "# Identifying the toxic word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mHDiHSfeHoA"
      },
      "source": [
        "### Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz1HXfT5eHoA"
      },
      "outputs": [],
      "source": [
        "toxic_word_path='../data/vocab/negative_words.txt'\n",
        "\n",
        "with open(toxic_word_path, 'r') as f:\n",
        "    toxic_word_list = f.read().splitlines()\n",
        "toxic_word_list = [line.strip() for line in toxic_word_list]\n",
        "\n",
        "def bag_of_words(sentence, list=toxic_word_list):\n",
        "\n",
        "    sentence_list = sentence.split()\n",
        "\n",
        "    for word in sentence_list:\n",
        "        if word in list:\n",
        "            sentence = sentence.replace(word, \"[MASK]\")\n",
        "\n",
        "\n",
        "    return sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULeGt3gNeHoB"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEGI_fIoeHoB"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "from sklearn.pipeline import make_pipeline\n",
        "pipe = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzQYCUJ9eHoC"
      },
      "outputs": [],
      "source": [
        "X_train = corpus_tox + corpus_norm\n",
        "y_train = [1] * len(corpus_tox) + [0] * len(corpus_norm)\n",
        "pipe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtU47D79eHoC"
      },
      "outputs": [],
      "source": [
        "coefs = pipe[1].coef_[0]\n",
        "coefs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux-ypmZ4eHoC"
      },
      "outputs": [],
      "source": [
        "word2coef = {w: coefs[idx] for w, idx in pipe[0].vocabulary_.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SutjeSFaeHoD"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(VOCAB_DIRNAME + '/word2coef.pkl', 'wb') as f:\n",
        "    pickle.dump(word2coef, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch6xe284eHoD"
      },
      "source": [
        "### Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO2_TtpoeHoD"
      },
      "outputs": [],
      "source": [
        "def mask(sentence):\n",
        "  # sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
        "  \n",
        "  sentence = ' ' + sentence + ' '\n",
        "  sentence = sentence.lower()\n",
        "  # adding spaces around punctuation\n",
        "  sentence = re.sub(r'([.,!?()])', r' \\1 ', sentence)\n",
        "\n",
        "  sentence = sentence.split(' ')\n",
        "\n",
        "  # true_sent = []\n",
        "  # for i in sentence:\n",
        "  #   true_sent.append(i)\n",
        "  # true_sent = sentence\n",
        "  true_sent = sentence.copy()\n",
        "\n",
        "  # print(true_sent)\n",
        "  masked_sentence = ''\n",
        "  min_tox = 1\n",
        "  while True:\n",
        "    for i in range(len(sentence)):\n",
        "\n",
        "      # print('1:',i,sentence)\n",
        "      sentence[i] = '[MASK]'\n",
        "      # print('2:',i,sentence)\n",
        "      sentence = ' '.join(sentence)\n",
        "      batch = tokenizer_roberta.encode(sentence, return_tensors='pt')\n",
        "      output = torch.nn.functional.softmax(model_roberta(batch).logits, dim = -1)\n",
        "      toxic_score = float(output[0][1])\n",
        "      if min_tox > toxic_score:\n",
        "        min_tox = toxic_score\n",
        "        masked_sentence = sentence\n",
        "\n",
        "      # print('nvwubrb',true_sent)\n",
        "      sentence = true_sent.copy()\n",
        "\n",
        "    \n",
        "    # if min_tox < 0.25:\n",
        "    #   break\n",
        "    break\n",
        "    \n",
        "    true_sent = masked_sentence\n",
        "    true_sent = true_sent.split(' ')\n",
        "  #join the sentence\n",
        "  \n",
        "  \n",
        "  return masked_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xLBgwp8pfb57",
        "outputId": "5d437872-8ff4-473c-e55c-962c072e0778"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' i [MASK] love you .  '"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask(\"I fucking love you.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRnu6fGHeHoE"
      },
      "source": [
        "## Use the fine-tuned BERT to predict alternatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX__DwFpeHoE"
      },
      "source": [
        "### BERT Model: Random mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CytFCcTJeHoF"
      },
      "outputs": [],
      "source": [
        "#loading the model\n",
        "def predictor_random(sentence, N):\n",
        "    inputs = tokenizer.batch_encode_plus([sentence], return_tensors='pt')\n",
        "    inputs[\"token_type_ids\"] = torch.zeros(len(inputs.input_ids)).to(torch.int64) # non-toxic\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model_random(**inputs).logits\n",
        "    \n",
        "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "    values, indices = torch.topk(logits[0, mask_token_index, :], N)\n",
        "\n",
        "    sentences_list = []\n",
        "    \n",
        "    for i in range(N):\n",
        "        inputs.input_ids[0][mask_token_index.item()] = indices[0][i]\n",
        "        sentences_list.append(tokenizer.decode(inputs.input_ids[0]))\n",
        "        # sentences_list.append()\n",
        "    \n",
        "    # sentence = sentence[:sentence.find('[MASK]')] + \" [MASK] \" + sentence[sentence.find('[MASK]'):]\n",
        "\n",
        "    # inputs = tokenizer.batch_encode_plus([sentence], return_tensors='pt')\n",
        "    # inputs[\"token_type_ids\"] = torch.zeros(len(inputs.input_ids)).to(torch.int64) # non-toxic\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     logits = model_random(**inputs).logits\n",
        "    \n",
        "    # mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "    # values, indices = torch.topk(logits[0, mask_token_index, :], N)\n",
        "\n",
        "    # # print(indices)\n",
        "\n",
        "    # for i in range(N):  \n",
        "    #     inputs.input_ids[0][mask_token_index] = torch.tensor([indices[0][i],indices[1][i]])\n",
        "    #     sentences_list.append(tokenizer.decode(inputs.input_ids[0]))\n",
        "    #     # sentences_list.append()\n",
        "\n",
        "\n",
        "\n",
        "    return sentences_list\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVxNaXcTeHoF"
      },
      "source": [
        "### BERT Model: Targetted mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQIew2G_eHoF"
      },
      "outputs": [],
      "source": [
        "def predictor_targeted(sentence, N):\n",
        "    inputs = tokenizer.batch_encode_plus([sentence], return_tensors='pt')\n",
        "    inputs[\"token_type_ids\"] = torch.zeros(len(inputs.input_ids)).to(torch.int64) # non-toxic\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model_targeted(**inputs).logits\n",
        "    \n",
        "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "    values, indices = torch.topk(logits[0, mask_token_index, :], N)\n",
        "\n",
        "    sentences_list = []\n",
        "    # replace every mask token by the top N tokens\n",
        "    # and add the sentences to the list\n",
        "\n",
        "    \n",
        "    for i in range(N):\n",
        "        inputs.input_ids[0][mask_token_index.item()] = indices[0][i]\n",
        "        sentences_list.append(tokenizer.decode(inputs.input_ids[0]))\n",
        "        # sentences_list.append()\n",
        "\n",
        "    \n",
        "    # sentence = sentence[:sentence.find('[MASK]')] + \" [MASK] \" + sentence[sentence.find('[MASK]'):]\n",
        "\n",
        "    # inputs = tokenizer.batch_encode_plus([sentence], return_tensors='pt')\n",
        "    # inputs[\"token_type_ids\"] = torch.zeros(len(inputs.input_ids)).to(torch.int64) # non-toxic\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     logits = model_random(**inputs).logits\n",
        "    \n",
        "    # mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "    # values, indices = torch.topk(logits[0, mask_token_index, :], N)\n",
        "\n",
        "    # # print(indices)\n",
        "\n",
        "    # for i in range(N):  \n",
        "    #     inputs.input_ids[0][mask_token_index] = torch.tensor([indices[0][i],indices[1][i]])\n",
        "    #     sentences_list.append(tokenizer.decode(inputs.input_ids[0]))\n",
        "    #     # sentences_list.append()\n",
        "\n",
        "    return sentences_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX5tmlL-eHoF"
      },
      "source": [
        "## Ranking based on Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJs9S911eHoG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ranker( possible_sentences,original_sentence):\n",
        "  max_score = -1\n",
        "\n",
        "  detoxed_sentence = ''\n",
        "  max_sim = 0\n",
        "  max_tox = 0\n",
        "  max_flu = 0\n",
        "\n",
        "  for sentence in possible_sentences:\n",
        "\n",
        "    sim = similarity_scorer(sentence, original_sentence)\n",
        "    tox = 1/toxicity_scorer(sentence)\n",
        "    flu = 1/fluency_scorer(sentence)\n",
        "    \n",
        "    # sent_score = similarity_scorer(sentence, original_sentence)/(toxicity_scorer(sentence)*fluency_scorer(sentence))\n",
        "    sent_score = sim*tox*flu\n",
        "\n",
        "    if sent_score > max_score:\n",
        "      detoxed_sentence = sentence\n",
        "      max_sim = sim\n",
        "      max_tox = tox\n",
        "      max_flu = flu\n",
        "\n",
        "  print(\"Similarity: \", max_sim)\n",
        "  print(\"Toxicity: \", max_tox)\n",
        "  print(\"Fluency: \", max_flu)    \n",
        "  \n",
        "    \n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-FRQETejnsx",
        "outputId": "fa50aa62-6b96-41dd-87bc-aa145d19c6ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity:  0.628970902033151\n",
            "Toxicity:  26007.29425990197\n",
            "Fluency:  0.009653532798826303\n",
            "[CLS] the entire article was disputed. [SEP]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"  the entire article was shit . \"\n",
        "print(ranker( predictor_targeted(mask(sentence),5), sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZlo_knmlid3",
        "outputId": "3cec67cc-b9f5-4b63-9826-2a5e43fe88c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity:  0.628970902033151\n",
            "Toxicity:  24755.113340881442\n",
            "Fluency:  0.010370140401029609\n",
            "[CLS] the entire article was reported. [SEP]\n"
          ]
        }
      ],
      "source": [
        "print(ranker( predictor_random(mask(sentence),5), sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoHCtgmieHoG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def Union(lst1, lst2):\n",
        "    final_list = lst1 + lst2\n",
        "    return final_list\n",
        "\n",
        "\n",
        "    \n",
        "def similarity_scorer(predicted_sentence, original_sentence):\n",
        "    #cosine similarity\n",
        "  l1=[]\n",
        "  l2=[]\n",
        "\n",
        "  list1 = word_tokenize(predicted_sentence)\n",
        "  list2 = word_tokenize(original_sentence)\n",
        "\n",
        "  list1 = [w for w in list1 if w not in sw]\n",
        "  list2 = [w for w in list2 if w not in sw]\n",
        "\n",
        "  rvector = Union(list1,list2)\n",
        "\n",
        "  for w in rvector:\n",
        "    if w in list1: l1.append(1)\n",
        "    else: l1.append(0)\n",
        "    if w in list2: l2.append(1)\n",
        "    else: l2.append(0)\n",
        "\n",
        "  c = 0\n",
        "\n",
        "  for i in range(len(rvector)):\n",
        "    c+=l1[i]*l2[i]\n",
        "  \n",
        "  cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "\n",
        "  return cosine\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6-1yX2IeHoG"
      },
      "outputs": [],
      "source": [
        "def toxicity_scorer(sentence):\n",
        "    #toxicity score\n",
        "  batch = tokenizer_roberta.encode(sentence, return_tensors='pt')\n",
        "  output = torch.nn.functional.softmax(model_roberta(batch).logits, dim = -1)\n",
        "  toxic_score = float(output[0][1])\n",
        "  return toxic_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ybw6IYseHoG"
      },
      "outputs": [],
      "source": [
        "def fluency_scorer(sentence):\n",
        "    tokenize_input = tokenizer_gpt.encode(sentence)\n",
        "    tensor_input = torch.tensor([tokenize_input])\n",
        "    loss=model_gpt(tensor_input, labels=tensor_input)[0]\n",
        "    return np.exp(loss.detach().numpy())\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JNT-rSUiO7-",
        "outputId": "f4dffa60-f046-40c6-9e81-edb46809a70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8\n",
            "4.7692290536360815e-05\n",
            "171.80545\n"
          ]
        }
      ],
      "source": [
        "sentence = \" i really love you . \"\n",
        "\n",
        "print(similarity_scorer(sentence,\" i fucking love you . \"))\n",
        "print(toxicity_scorer(sentence))\n",
        "print(fluency_scorer(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing the model on jigsaw database\n",
        "\n",
        "import pickle\n",
        "test = pickle.load('../data/test.csv', 'rb')\n",
        "\n",
        "score_random = 0\n",
        "score_targeted = 0\n",
        "\n",
        "for i in test:\n",
        "    score_random += (ranker( predictor_random(mask(sentence),5), sentence), sentence)\n",
        "    score_targeted += (ranker( predictor_targeted(mask(sentence),5), sentence), sentence)\n",
        "\n",
        "print(score_random/len(test))\n",
        "print(score_targeted/len(test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('nlp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "db9d433ab76aa2bebc7886b62d36feb1292e7830d52ec4e11c27f277e0aea8dc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
