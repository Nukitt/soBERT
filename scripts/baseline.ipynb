{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCbgiuBmpydK"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/skoltech-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip\n",
        "!unzip filtered_paranmt.zip"
      ],
      "metadata": {
        "id": "aYNcs6ZqqBlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yFFaS6O_qFep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('filtered.tsv', sep='\\t', encoding='utf-8')\n",
        "print(df.shape)\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "7mOrygSGqI4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df.ref_tox > df.trn_tox).mean()"
      ],
      "metadata": {
        "id": "8Q-njONKqPSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx = []\n",
        "yy = []\n",
        "for i, row in df.iterrows():\n",
        "    if row.ref_tox > row.trn_tox:\n",
        "        xx.append(row.reference)\n",
        "        yy.append(row.translation)\n",
        "    else:\n",
        "        yy.append(row.reference)\n",
        "        xx.append(row.translation)\n",
        "        \n",
        "xydf = pd.DataFrame({'source': xx, 'target': yy})"
      ],
      "metadata": {
        "id": "W0xHL9J0qPQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer, T5TokenizerFast,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "metadata": {
        "id": "9XwR3EkSqPOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t5Para = \"ceshine/t5-paraphrase-paws-msrp-opinosis\""
      ],
      "metadata": {
        "id": "ebaKdra3qPLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5TokenizerFast.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "eB2ZFr3bqOc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "DffCZWa9qKIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(xydf, test_size=300)\n",
        "print(df_train.shape[0], df_test.shape[0])"
      ],
      "metadata": {
        "id": "EX1mELrVqY66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "x1 = tokenizer(df_train.source.tolist(), truncation=True)\n",
        "y1 = tokenizer(df_train.target.tolist(), truncation=True)\n",
        "x2 = tokenizer(df_test.source.tolist(), truncation=True)\n",
        "y2 = tokenizer(df_test.target.tolist(), truncation=True)"
      ],
      "metadata": {
        "id": "GFozq4umqZyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PairsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx < len(self.x['input_ids'])\n",
        "        item = {key: val[idx] for key, val in self.x.items()}\n",
        "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
        "        item['labels'] = self.y['input_ids'][idx]\n",
        "        return item\n",
        "    \n",
        "    @property\n",
        "    def n(self):\n",
        "        return len(self.x['input_ids'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n # * 2\n",
        "    \n",
        "train_dataset = PairsDataset(x1, y1)\n",
        "test_dataset = PairsDataset(x2, y2)\n",
        "len(train_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "Im-3Cc69qcN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "GZHaK2DBqdyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=4, drop_last=True, shuffle=True, num_workers=1)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, drop_last=True, shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "id": "rAei2eO9qekW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "import torch"
      ],
      "metadata": {
        "id": "c7KzTogvqgFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_name = 'SkolkovoInstitute/t5-paraphrase-paws-msrp-opinosis-paranmt'"
      ],
      "metadata": {
        "id": "S-ckh9CeqhEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(checkpoint_name)"
      ],
      "metadata": {
        "id": "1Sb8tHoDqjD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "MB08mDTJqkxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.file_utils import cached_property\n",
        "from typing import Tuple\n",
        "\n",
        "class TrAr(TrainingArguments):\n",
        "    @cached_property\n",
        "    def _setup_devices(self):\n",
        "        return device"
      ],
      "metadata": {
        "id": "Ph4wGDmTqmzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Union\n",
        "\n",
        "class DataCollatorWithPadding:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        batch = self.tokenizer.pad(\n",
        "            features,\n",
        "            padding=True,\n",
        "        )\n",
        "        ybatch = self.tokenizer.pad(\n",
        "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
        "            padding=True,\n",
        "        ) \n",
        "        batch['labels'] = ybatch['input_ids']\n",
        "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
        "        \n",
        "        return {k: torch.tensor(v) for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "N4Sb4PohqnSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_name = 'models/t5-cechine-nmt-mined-detox'\n",
        "training_args = TrAr(\n",
        "    output_dir=save_name,   # output directory\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,             # total # of training epochs\n",
        "    per_device_train_batch_size=4,  # batch size per device during training\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
        "    warmup_steps=300,               # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0,                  # strength of weight decay\n",
        "    learning_rate=3e-5,\n",
        "    logging_dir='./logs',           # directory for storing logs\n",
        "    logging_steps=100,\n",
        "    eval_steps=100,\n",
        "    evaluation_strategy='steps',\n",
        "    save_total_limit=1,\n",
        "    save_steps=5000,\n",
        ")"
      ],
      "metadata": {
        "id": "Y46A0WEaqpy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "OF4prEGzqqnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache();"
      ],
      "metadata": {
        "id": "y1E_7FoXqsqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "n_YH78JsquC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "NaBWrtJXqwHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval();"
      ],
      "metadata": {
        "id": "-7wyZYmIqytS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(t5Para)"
      ],
      "metadata": {
        "id": "Bf1GBKphqy_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('hey loser , try this get a fucking life and stay out of mine , which you know nothing about', return_tensors='pt')\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "for t in model.generate(**inputs, num_return_sequences=10, do_sample=False, num_beams=10):\n",
        "    print(tokenizer.decode(t, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "d_fO11-1rbXH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}